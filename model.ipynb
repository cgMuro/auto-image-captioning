{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We want to create:\n",
    "* a set that contains all the photos names\n",
    "* a dictionary that maps the names of the photos to the descriptions\n",
    "* another dictionary that maps the names of the photos to the features extracted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Photo ids set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    # Open file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Divide the data into each photo's name\n",
    "    data = data.split('\\n')\n",
    "    # Init result set\n",
    "    result = set()\n",
    "    # Loop through each photo's name\n",
    "    for line in data:\n",
    "        # Avoid empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # Remove .jpg and add to result set\n",
    "        result.add(line.split('.')[0])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "source": [
    "### Dictionary that maps photo's id to descriptions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(filename, ids):\n",
    "    # Open file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Divide the data into each photo description    \n",
    "    data = data.split('\\n')\n",
    "    # Init result dictionary\n",
    "    result = dict()\n",
    "\n",
    "    # Loop through each line in the data\n",
    "    for line in data:\n",
    "        # Divide photo id and description\n",
    "        line = line.split()\n",
    "        photo_id = line[0]\n",
    "        description = ' '.join(line[1:])\n",
    "\n",
    "        # Check if photo id is in ids\n",
    "        if photo_id in ids:\n",
    "            # Check if this photo id key has already been initiated\n",
    "            if photo_id not in result:\n",
    "                # Set the value to a list\n",
    "                result[photo_id] = list()\n",
    "\n",
    "            # Add to the description a start and end sequence tokens\n",
    "            description = 'STARTSEQ ' + description + ' ENDSEQ'\n",
    "\n",
    "            # Add description to list of values of the corresponding photo id\n",
    "            result[photo_id].append(description)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "source": [
    "### Dictionary that maps photo's id to features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(filename, ids):\n",
    "    # Load the file that contains all the feature\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    # Map every photo id to the features saved inside the file\n",
    "    result = {k: data[k] for k in ids}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "source": [
    "### Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Number of photo ids: {len(photo_ids)}, Number of key-value pairs of photos and descriptions: {len(photo_to_descriptions)}, Number of key-value pairs of photos and features: {len(photo_to_features)}\")\n",
    "# print('')\n",
    "# print('Example of the content of photo ids:', next(iter(photo_ids)))\n",
    "# print('')\n",
    "# print('Example of the content of photo ids to descriptions:', photo_to_descriptions['1859726819_9a793b3b44'])\n",
    "# print('')\n",
    "# print('Example of the content of photo ids to features:', photo_to_features['1859726819_9a793b3b44'])"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_descs(data):\n",
    "    all_descriptions = []\n",
    "    for i in data.values():\n",
    "        for j in i:\n",
    "            all_descriptions.append(j)\n",
    "            \n",
    "    return all_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(data):\n",
    "    all_descriptions = all_descs(data)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_descriptions)\n",
    "    return tokenizer"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(tokenizer, maxlength, photos_to_descriptions, photos_to_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    for k, v in photos_to_descriptions.items():\n",
    "        for desc in v:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=maxlength)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1.append(photos_to_features[k][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Train Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_ids_train = load_data('./Flickr8k_text/Flickr_8k.trainImages.txt')\n",
    "\n",
    "photo_to_descriptions_train = load_descriptions('./descriptions.txt', ids=photo_ids_train)\n",
    "\n",
    "photo_to_features_train = load_features('./features.pkl', ids=photo_ids_train)\n",
    "\n",
    "tokenizer_train = create_tokenizer(photo_to_descriptions_train)\n",
    "vocab_size_train = len(tokenizer_train.word_index) + 1\n",
    "\n",
    "# Number of words in the longest description\n",
    "maxlength_train = max(len(d.split()) for d in all_descs(photo_to_descriptions_train))\n",
    "\n",
    "X1train, X2train, ytrain = create_sequence(tokenizer_train, maxlength_train, photo_to_descriptions_train, photo_to_features_train, vocab_size_train)"
   ]
  },
  {
   "source": [
    "## Test Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_ids_test = load_data('./Flickr8k_text/Flickr_8k.devImages.txt')\n",
    "\n",
    "photo_to_descriptions_test = load_descriptions('./descriptions.txt', ids=photo_ids_test)\n",
    "\n",
    "photo_to_features_test = load_features('./features.pkl', ids=photo_ids_test)\n",
    "\n",
    "tokenizer_test = create_tokenizer(photo_to_descriptions_test)\n",
    "vocab_size_test = len(tokenizer_test.word_index) + 1\n",
    "\n",
    "# Number of words in the longest description\n",
    "maxlength_test = max(len(d.split()) for d in all_descs(photo_to_descriptions_test))\n",
    "\n",
    "X1test, X2test, ytest = create_sequence(tokenizer_test, maxlength_test, photo_to_descriptions_test, photo_to_features_test, vocab_size_test)"
   ]
  },
  {
   "source": [
    "# Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Structure of the model:\n",
    "* ```Photo Features Extractor``` --> the <em>pre-trained VGG16 model</em> (the one we used to pre-process the images features)\n",
    "* ``Sequence Processor`` --> <em>word embedding layer</em> + <em>LSTM layer</em>\n",
    "* ``Decoder`` --> it process the outputs of the **Photo Feature Extractor** and **Sequence Processor** and merges them together using a Dense layer to make a prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 37)]         0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 4096)]       0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 37, 256)      1943808     input_4[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 37, 256)      0           embedding_1[0][0]                \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n                                                                 lstm_1[0][0]                     \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 7593)         1951401     dense_4[0][0]                    \n==================================================================================================\nTotal params: 5,535,145\nTrainable params: 5,535,145\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "# Features extraction model\n",
    "inputs1 = tf.keras.Input(shape=(4096,))\n",
    "feature_extraction_1 = tf.keras.layers.Dropout(0.5)(inputs1)\n",
    "feature_extraction_2 = tf.keras.layers.Dense(256, activation='relu')(feature_extraction_1)\n",
    "\n",
    "# Sequence processor model\n",
    "inputs2 = tf.keras.Input(shape=(maxlength_train, ))\n",
    "sequence_processor_1 = tf.keras.layers.Embedding(vocab_size_train, 256, mask_zero=True)(inputs2)\n",
    "sequence_processor_2 = tf.keras.layers.Dropout(0.5)(sequence_processor_1)\n",
    "sequence_processor_3 = tf.keras.layers.LSTM(256)(sequence_processor_2)\n",
    "\n",
    "# Decoder model\n",
    "decoder_1 = tf.keras.layers.add([feature_extraction_2, sequence_processor_3])\n",
    "decoder_2 = tf.keras.layers.Dense(256, activation='relu')(decoder_1)\n",
    "outputs = tf.keras.layers.Dense(vocab_size_train, activation='softmax')(decoder_2)\n",
    "\n",
    "# Put everything together\n",
    "model = tf.keras.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "#print(tf.keras.utils.plot_model(model, show_shapes=True))"
   ]
  },
  {
   "source": [
    "### Fit the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We start by defining the filepath of the save model and when the model should be saved (which means when the model achives some results)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './models/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then fit the model\n",
    "#model.fit([X1train, X2train], ytrain, epochs=1, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "source": [
    "### Train with progessive overloading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_progressive_overloading(tokenizer, maxlength, desc_list, photos_to_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=maxlength)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X1.append(photos_to_features)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(photos_to_descriptions, photos_to_features, tokenizer, maxlength, vocab_size):\n",
    "    # Loop forever over images\n",
    "    while True:\n",
    "        for k, v in photos_to_descriptions.items():\n",
    "            photo = photos_to_features[k][0]\n",
    "            in_img, in_seq, out_word = create_sequence_progressive_overloading(tokenizer, maxlength, v, photo, vocab_size)\n",
    "            yield ([np.array(in_img), np.array(in_seq)], np.array(out_word))"
   ]
  },
  {
   "source": [
    "# Train the model\n",
    "epochs = 1\n",
    "steps = len(photo_to_descriptions_train)\n",
    "for i in range(epochs):\n",
    "    # Create generator\n",
    "    generator = data_generator(photo_to_descriptions_train, photo_to_features_train, tokenizer_train, maxlength_train, vocab_size_train)\n",
    "    # Fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # Save model\n",
    "    model.save('./models/model_' + str(i) + '.h5')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6000/6000 [==============================] - 1792s 299ms/step - loss: 3.8758\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that given an integer returns the corresponding word based on the tokenization\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(model, tokenizer, photo, maxlength):\n",
    "    # Word that starts the sequence\n",
    "    in_text = 'STARTSEQ'\n",
    "    # Iterate over the sequence\n",
    "    for i in range(maxlength):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=maxlength)\n",
    "        # Predict next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # Get the integer with the biggest probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Get the word corresponding to the integer the model returned\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # Stop if no valid word was predicted\n",
    "        if word is None:\n",
    "            break\n",
    "        # Append the predicted word as input for the next word\n",
    "        in_text += ' ' + word\n",
    "        # Stop if we predicted the end of the sequence\n",
    "        if word.upper() == 'ENDSEQ':\n",
    "            break\n",
    "\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the skill of the model\n",
    "def evaluate_model(model, photos_to_descriptions, photos_to_features, tokenizer, maxlength):\n",
    "    actual, predicted = [], []\n",
    "\n",
    "    for k, v in photos_to_descriptions.items():\n",
    "        # Generate description\n",
    "        yhat = generate_description(model, tokenizer, photos_to_features[k], maxlength)\n",
    "\n",
    "        references = [d.split() for d in v]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    \n",
    "        # Calculate BLEU score\n",
    "        # BLEU scores are used in text translation for evaluating translated text against one or more reference translations\n",
    "        print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "        print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "        print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "        print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "source": [
    "## Test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU-1: 0.184211\n",
      "BLEU-2: 0.099786\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "BLEU-1: 0.184211\n",
      "BLEU-2: 0.122213\n",
      "BLEU-3: 0.096690\n",
      "BLEU-4: 0.000000\n",
      "BLEU-1: 0.166667\n",
      "BLEU-2: 0.102521\n",
      "BLEU-3: 0.077049\n",
      "BLEU-4: 0.000000\n",
      "BLEU-1: 0.157895\n",
      "BLEU-2: 0.097988\n",
      "BLEU-3: 0.068787\n",
      "BLEU-4: 0.000000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-1b71032df5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./features.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-e88ae04c4f54>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, photos_to_descriptions, photos_to_features, tokenizer, maxlength)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphotos_to_descriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Generate description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphotos_to_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-c09555699705>\u001b[0m in \u001b[0;36mgenerate_description\u001b[0;34m(model, tokenizer, photo, maxlength)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Predict next word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Get the integer with the biggest probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1593\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n",
      "\u001b[0;32m~/Desktop/Python/scripts/projects/AI/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3003\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m         tld.op_callbacks, dataset, iterator)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = load_data('./Flickr8k_text/Flickr_8k.testImages.txt')\n",
    "\n",
    "test_desc = load_descriptions('./descriptions.txt', test)\n",
    "test_features = load_features('./features.pkl', test)\n",
    "\n",
    "evaluate_model(model, test_desc, test_features, tokenizer_train, maxlength_train)"
   ]
  },
  {
   "source": [
    "# New predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* We can save the tokenizer with pickle, and then load it as well as the model\n",
    "* We extract the features\n",
    "* We generate a description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file):\n",
    "    print('Extracting features...')\n",
    "\n",
    "    # Load model\n",
    "    model = VGG16()\n",
    "    # Re-structure the model (we remove the last layer from the model because we don't need to classify the photos)\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    # Load image from file\n",
    "    image = load_img(file, target_size=(224, 224))\n",
    "    # Convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # Reshape image for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # Prepare image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # Extract features\n",
    "    features = model.predict(image, verbose=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting features...\n",
      "STARTSEQ a man in a red shirt is standing on a street endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq endseq\n"
     ]
    }
   ],
   "source": [
    "photo = extract_features('example.jpg')\n",
    "description = generate_desc(model, tokenizer_train, photo, maxlength_train)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}