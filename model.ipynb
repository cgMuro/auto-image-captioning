{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We want to create:\n",
    "* a set that contains all the photos names\n",
    "* a dictionary that maps the names of the photos to the descriptions\n",
    "* another dictionary that maps the names of the photos to the features extracted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Photo ids set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    # Open file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Divide the data into each photo's name\n",
    "    data = data.split('\\n')\n",
    "    # Init result set\n",
    "    result = set()\n",
    "    # Loop through each photo's name\n",
    "    for line in data:\n",
    "        # Avoid empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # Remove .jpg and add to result set\n",
    "        result.add(line.split('.')[0])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "source": [
    "### Dictionary that maps photo's id to descriptions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(filename, ids):\n",
    "    # Open file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Divide the data into each photo description    \n",
    "    data = data.split('\\n')\n",
    "    # Init result dictionary\n",
    "    result = dict()\n",
    "\n",
    "    # Loop through each line in the data\n",
    "    for line in data:\n",
    "        # Divide photo id and description\n",
    "        line = line.split()\n",
    "        photo_id = line[0]\n",
    "        description = ' '.join(line[1:])\n",
    "\n",
    "        # Check if photo id is in ids\n",
    "        if photo_id in ids:\n",
    "            # Check if this photo id key has already been initiated\n",
    "            if photo_id not in result:\n",
    "                # Set the value to a list\n",
    "                result[photo_id] = list()\n",
    "\n",
    "            # Add to the description a start and end sequence tokens\n",
    "            description = 'STARTSEQ ' + description + ' ENDSEQ'\n",
    "\n",
    "            # Add description to list of values of the corresponding photo id\n",
    "            result[photo_id].append(description)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "source": [
    "### Dictionary that maps photo's id to features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(filename, ids):\n",
    "    # Load the file that contains all the feature\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    # Map every photo id to the features saved inside the file\n",
    "    result = {k: data[k] for k in ids}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "source": [
    "# GloVe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_descs(data):\n",
    "    all_descriptions = []\n",
    "    for i in data.values():\n",
    "        for j in i:\n",
    "            all_descriptions.append(j)\n",
    "            \n",
    "    return all_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(data):\n",
    "    all_descriptions = all_descs(data)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_descriptions)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "def create_emb_dict():\n",
    "  with open(f'glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings dictionary\n",
    "def save_embeddings(save=True):\n",
    "    if save:\n",
    "        pickle.dump(embeddings_dict, open(f'./saved_data/embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_emb(tokenizer, vocab_size):\n",
    "  all_embs = np.stack(embeddings_dict.values())\n",
    "  emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "  embed_size = all_embs.shape[1]\n",
    "\n",
    "\n",
    "  word_index = tokenizer.word_index\n",
    "\n",
    "  embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embed_size))\n",
    "\n",
    "  for word, i in word_index.items():\n",
    "      embedding_vector = embeddings_dict.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "\n",
    "  return embed_size, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(tokenizer, maxlength, photos_to_descriptions, photos_to_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    for k, v in photos_to_descriptions.items():\n",
    "        for desc in v:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=maxlength)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1.append(photos_to_features[k][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Prepare for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_ids_train = load_data('./Flickr8k_text/Flickr_8k.trainImages.txt')\n",
    "\n",
    "photo_to_descriptions_train = load_descriptions('./saved_data/descriptions.txt', ids=photo_ids_train)\n",
    "\n",
    "photo_to_features_train = load_features('./saved_data/features.pkl', ids=photo_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(photo_to_descriptions_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Number of words in the longest description\n",
    "maxlength = max(len(d.split()) for d in all_descs(photo_to_descriptions_train))"
   ]
  },
  {
   "source": [
    "## Glove set up"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings Dictionary\n",
    "LOAD_EMBEDDINGS = True\n",
    "if LOAD_EMBEDDINGS:\n",
    "    embeddings_dict = pickle.load(open('./saved_data/embeddings.pkl', 'rb'))\n",
    "else:\n",
    "    create_emb_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "embed_size, embedding_matrix = init_emb(tokenizer=tokenizer, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(save=False)"
   ]
  },
  {
   "source": [
    "# Building the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Structure of the model:\n",
    "* ```Photo Features Extractor``` --> the <em>pre-trained VGG16 model</em> (the one we used to pre-process the images features)\n",
    "* ``Sequence Processor`` --> <em>word embedding layer</em> + <em>LSTM layer</em>\n",
    "* ``Decoder`` --> it process the outputs of the **Photo Feature Extractor** and **Sequence Processor** and merges them together using a Dense layer to make a prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 37)]         0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 4096)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 37, 100)      759300      input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 37, 100)      0           embedding[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 256)          365568      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256)          0           dense[0][0]                      \n                                                                 lstm[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 7593)         1951401     dense_1[0][0]                    \n==================================================================================================\nTotal params: 4,190,893\nTrainable params: 3,431,593\nNon-trainable params: 759,300\n__________________________________________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "# Features extraction model\n",
    "inputs1 = tf.keras.Input(shape=(4096,))\n",
    "feature_extraction_1 = tf.keras.layers.Dropout(0.5)(inputs1)\n",
    "feature_extraction_2 = tf.keras.layers.Dense(256, activation='relu')(feature_extraction_1)\n",
    "\n",
    "# Sequence processor model\n",
    "inputs2 = tf.keras.Input(shape=(maxlength, ))\n",
    "sequence_processor_1 = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], mask_zero=True, weights=[embedding_matrix], trainable=False)(inputs2)\n",
    "sequence_processor_2 = tf.keras.layers.Dropout(0.5)(sequence_processor_1)\n",
    "sequence_processor_3 = tf.keras.layers.LSTM(256)(sequence_processor_2)\n",
    "\n",
    "# Decoder model\n",
    "decoder_1 = tf.keras.layers.add([feature_extraction_2, sequence_processor_3])\n",
    "decoder_2 = tf.keras.layers.Dense(256, activation='relu')(decoder_1)\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder_2)\n",
    "\n",
    "# Put everything together\n",
    "model = tf.keras.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "#print(tf.keras.utils.plot_model(model, show_shapes=True))"
   ]
  },
  {
   "source": [
    "## Train with progessive overloading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We use **progressing overloading** to reduce the request of resources to the computer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_progressive_overloading(tokenizer, maxlength, desc_list, photos_to_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=maxlength)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X1.append(photos_to_features)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(photos_to_descriptions, photos_to_features, tokenizer, maxlength, vocab_size):\n",
    "    # Loop forever over images\n",
    "    while True:\n",
    "        for k, v in photos_to_descriptions.items():\n",
    "            photo = photos_to_features[k][0]\n",
    "            in_img, in_seq, out_word = create_sequence_progressive_overloading(tokenizer, maxlength, v, photo, vocab_size)\n",
    "            yield ([np.array(in_img), np.array(in_seq)], np.array(out_word))"
   ]
  },
  {
   "source": [
    "# Train the model\n",
    "epochs = 30\n",
    "steps = len(photo_to_descriptions_train)\n",
    "for i in range(epochs):\n",
    "    # Create generator\n",
    "    generator = data_generator(photo_to_descriptions_train, photo_to_features_train, tokenizer, maxlength, vocab_size)\n",
    "    # Fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # Save model in every iteration\n",
    "    model.save('./models/model_' + str(i) + '.h5')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-4a14a6aefdf9>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "6000/6000 [==============================] - 1308s 218ms/step - loss: 4.2106\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that given an integer returns the corresponding word based on the tokenization\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(model, tokenizer, photo, maxlength):\n",
    "    # Word that starts the sequence\n",
    "    in_text = 'STARTSEQ'\n",
    "    # Iterate over the sequence\n",
    "    for i in range(maxlength):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=maxlength)\n",
    "        # Predict next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # Get the integer with the biggest probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Get the word corresponding to the integer the model returned\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # Stop if no valid word was predicted\n",
    "        if word is None:\n",
    "            break\n",
    "        # Append the predicted word as input for the next word\n",
    "        in_text += ' ' + word\n",
    "        # Stop if we predicted the end of the sequence\n",
    "        if word.upper() == 'ENDSEQ':\n",
    "            break\n",
    "\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the skill of the model\n",
    "def evaluate_model(model, photos_to_descriptions, photos_to_features, tokenizer, maxlength):\n",
    "    actual, predicted = [], []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for k, v in photos_to_descriptions.items():\n",
    "        # Generate description\n",
    "        yhat = generate_description(model, tokenizer, photos_to_features[k], maxlength)\n",
    "\n",
    "        references = [d.split() for d in v]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    \n",
    "        # Calculate BLEU score\n",
    "        # BLEU scores are used in text translation for evaluating translated text against one or more reference translations\n",
    "        print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "        print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "        print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "        print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "        print('')\n",
    "\n",
    "        # Keep track of the number of times the BLEU score was checked\n",
    "        if i == 2:\n",
    "            break\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "source": [
    "## Test the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU-1: 0.692308\n",
      "BLEU-2: 0.339683\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "\n",
      "BLEU-1: 0.624426\n",
      "BLEU-2: 0.337458\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "\n",
      "BLEU-1: 0.594369\n",
      "BLEU-2: 0.350178\n",
      "BLEU-3: 0.189938\n",
      "BLEU-4: 0.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = load_data('./Flickr8k_text/Flickr_8k.testImages.txt')\n",
    "\n",
    "test_desc = load_descriptions('./saved_data/descriptions.txt', test)\n",
    "test_features = load_features('./saved_data/features.pkl', test)\n",
    "\n",
    "evaluate_model(model, test_desc, test_features, tokenizer, maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}